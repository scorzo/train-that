{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmyqwt/3ki/sN/B2JtMmZx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scorzo/train-that/blob/main/train_that.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Setup Environment"
      ],
      "metadata": {
        "id": "igCsuGIBnPkk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsEi3U_BmwLq"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Import Libraries\n",
        "\n",
        "Next, import the required libraries:\n",
        "\n"
      ],
      "metadata": {
        "id": "LIvm6nOtnTgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "x2A6fW7wm4l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Load and Prepare the Data\n",
        "\n",
        "Load your CSV file and prepare it for training:"
      ],
      "metadata": {
        "id": "ybe_RoDxnYa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('sample_data/support_tickets_resolutions.csv')\n",
        "\n",
        "# Combine the issue and resolution into a single text for each record\n",
        "df['combined_text'] = df['Support Issue Description'] + \" [Resolution] \" + df['Support Resolution']\n",
        "\n",
        "# Save the combined text to a new file (required for the TextDataset)\n",
        "df['combined_text'].to_csv('training_text.txt', header=False, index=False)\n"
      ],
      "metadata": {
        "id": "9-F5fI7om7VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Prepare the Model and Tokenizer\n",
        "\n",
        "Load the GPT-2 model and tokenizer:\n",
        "\n"
      ],
      "metadata": {
        "id": "L-ANJ7lonklp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Additional step to add the special token [Resolution] to the tokenizer\n",
        "special_tokens_dict = {'additional_special_tokens': ['[Resolution]']}\n",
        "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "model.resize_token_embeddings(len(tokenizer))\n"
      ],
      "metadata": {
        "id": "rgjORdMVnAgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Prepare the Dataset for Training\n",
        "\n",
        "Create a dataset and data collator for training:"
      ],
      "metadata": {
        "id": "4I5dgoFTnxJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"training_text.txt\",\n",
        "    block_size=128\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "I9mjefaenDU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Training Arguments and Trainer\n",
        "\n",
        "Set up the training arguments and the trainer:"
      ],
      "metadata": {
        "id": "MylCzviqn00j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=3, #10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        ")\n"
      ],
      "metadata": {
        "id": "m8_a0fMnnIg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Fine-Tune the Model\n",
        "\n",
        "Finally, fine-tune the model:"
      ],
      "metadata": {
        "id": "kLamuIYrn7lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "x8-CB0q1nLXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: (Optional) Uploading the Model via Hugging Face Website\n",
        "\n",
        "\n",
        "Create an Account and Repository:\n",
        "\n",
        "Sign up or log in to Hugging Face.\n",
        "Create a new model repository.\n",
        "Prepare Your Model Locally:\n",
        "\n",
        "Ensure your model is saved locally with all necessary files (like config.json, pytorch_model.bin, etc.).\n",
        "Upload the Model:\n",
        "\n",
        "Navigate to your new repository on the Hugging Face website.\n",
        "Use the web interface to upload your model files directly. Typically, you'll be able to drag and drop files or use a file chooser."
      ],
      "metadata": {
        "id": "hDTW7vPi85rh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9: (Optional) Use Model for Inference"
      ],
      "metadata": {
        "id": "1vJfgjew9N1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Replace 'your-username/your-model-name' with the path to your model\n",
        "tokenizer = AutoTokenizer.from_pretrained('your-username/your-model-name')\n",
        "model = AutoModelForCausalLM.from_pretrained('your-username/your-model-name')\n",
        "\n",
        "# Example inference\n",
        "input_ids = tokenizer.encode('Your input text here', return_tensors='pt')\n",
        "output = model.generate(input_ids)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "DmmDZmE883SZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}